# ğŸ§  RAG-Document-Assistant  

A powerful Retrieval-Augmented Generation (RAG) system that allows users to upload documents and chat with them using semantic search, embeddings, chunking, and LLM-based answers.

This project is designed from **beginner setup to advanced production-level RAG architecture**.

---

## ğŸš€ Features

- ğŸ“‚ Upload files (PDF, TXT, DOCX, CSV, etc.)
- âœ‚ï¸ Intelligent text chunking
- ğŸ”¢ Embedding generation
- ğŸ§¬ Vector database storage
- ğŸ” Semantic search
- ğŸ¤– LLM-based answer generation
- ğŸ“Š Metadata-based filtering
- ğŸ—ƒï¸ Multi-document support
- ğŸ”„ Re-indexing & caching
- âš¡ FastAPI backend ready
- ğŸŒ Frontend-ready APIs

---

## ğŸ—ï¸ RAG Architecture Overview

```text
User Query
   â†“
Semantic Search (Vector DB)
   â†“
Top-k Relevant Chunks
   â†“
LLM Prompt + Context
   â†“
Final Answer


ğŸ› ï¸ Tech Stack
Layer	Tech
Backend	Python, FastAPI
LLM	OpenAI / Claude / Gemini / Local LLM
Embeddings	OpenAI / SentenceTransformers
Vector DB	FAISS / Chroma / Weaviate / Pinecone
Parsing	PyPDF, LangChain, Unstructured
Storage	Local / S3
Frontend	React / Next.js

ğŸ“¦ Project Structure
bash
Copy code
RAG-Document-Assistant/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/            # FastAPI routes
â”‚   â”œâ”€â”€ services/       # RAG logic
â”‚   â”œâ”€â”€ embeddings/     # Embedding models
â”‚   â”œâ”€â”€ vectorstore/    # FAISS/Chroma logic
â”‚   â”œâ”€â”€ loaders/        # File parsers
â”‚   â”œâ”€â”€ chunking/       # Text splitters
â”‚   â”œâ”€â”€ prompts/        # Prompt templates
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/
â”‚   â””â”€â”€ vector_db/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


âš™ï¸ Installation
bash
Copy code
git clone https://github.com/yourname/RAG-Document-Assistant.git
cd RAG-Document-Assistant

python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

pip install -r requirements.txt
ğŸ”‘ Environment Variables
Create .env

env
Copy code
OPENAI_API_KEY=your_api_key
EMBEDDING_MODEL=text-embedding-3-small
LLM_MODEL=gpt-4o-mini
ğŸ“‚ Step 1: Document Loading
python
Copy code
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("file.pdf")
docs = loader.load()


âœ‚ï¸ Step 2: Chunking Strategy
Why chunking?
LLMs cannot process long documents directly. Chunking improves retrieval accuracy.

python
Copy code
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100
)
chunks = splitter.split_documents(docs)
Best Practices:

chunk_size: 300â€“800 tokens

chunk_overlap: 10â€“20%

Smaller chunks = better precision

Larger chunks = better context

ğŸ”¢ Step 3: Embeddings
Embeddings convert text â†’ vectors.

python
Copy code
from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
Alternatives:

SentenceTransformers

BAAI/bge-large

all-MiniLM-L6-v2

ğŸ§¬ Step 4: Vector Store
FAISS Example:

python
Copy code
from langchain.vectorstores import FAISS

db = FAISS.from_documents(chunks, embeddings)
db.save_local("data/vector_db")
Chroma Example:

python
Copy code
from langchain.vectorstores import Chroma

db = Chroma.from_documents(chunks, embeddings, persist_directory="./chroma")
ğŸ” Step 5: Semantic Search
python
Copy code
query = "Explain transformer architecture"
docs = db.similarity_search(query, k=4)
Advanced:

python
Copy code
db.similarity_search_with_score(query)


ğŸ¤– Step 6: RAG Answer Generation
python
Copy code
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

llm = ChatOpenAI(temperature=0)
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever()
)

qa.run("What is attention mechanism?")


ğŸ§  Prompt Engineering (Advanced)
python
Copy code
SYSTEM_PROMPT = """
You are a document assistant.
Use only the given context.
If answer not found, say 'Not in documents'.
"""

ğŸ§© Advanced RAG Concepts
Concept	Description
Hybrid Search	Keyword + vector search
Re-ranking	Improve result accuracy
Metadata filtering	Filter by file, date, category
Context compression	Reduce irrelevant tokens
Agentic RAG	Tool-using RAG system
Streaming responses	Real-time UI updates
Multi-Vector Retrieval	Tables, code, images

ğŸ§  Chunk Optimization Techniques
Semantic chunking

Sliding window chunking

Markdown header chunking

Title-aware chunking

ğŸ—‚ï¸ Metadata Example
python
Copy code
doc.metadata = {
  "source": "report.pdf",
  "page": 3,
  "category": "finance"
}
Then:

python
Copy code
db.similarity_search(query, filter={"category": "finance"})


âš¡ Performance Tips
Cache embeddings

Batch processing

Async FastAPI routes

Use GPU for embedding

Index compression (HNSW, IVF)

ğŸŒ API Example
python
Copy code
POST /upload
POST /chat
GET  /documents
DELETE /documents/{id}

ğŸ›¡ï¸ Security
File validation

Size limits

Auth middleware

Rate limiting

Prompt injection protection

ğŸ“ˆ Future Enhancements
UI Dashboard

Drag & drop uploads

RAG evaluation metrics

Agentic workflows

OCR for scanned PDFs

Multilingual embeddings

venv- source .venv/bin/activate
